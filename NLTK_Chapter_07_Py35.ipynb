{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk, re, pprint\n",
    "\n",
    "docu = \" Hi there is there an entity in here , if yes please New York it. Also consider the value of the  City when talking about Property which is worth more than a Million Dollars \"\n",
    "\n",
    "def ie_preprocess(docu):\n",
    "    sentences = nltk.sent_tokenize(docu)\n",
    "    sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "    sentences1 = [nltk.pos_tag(sent) for sent in sentences]\n",
    "    \n",
    "    return sentences1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Hi', 'NNP'),\n",
       "  ('there', 'EX'),\n",
       "  ('is', 'VBZ'),\n",
       "  ('there', 'EX'),\n",
       "  ('an', 'DT'),\n",
       "  ('entity', 'NN'),\n",
       "  ('in', 'IN'),\n",
       "  ('here', 'RB'),\n",
       "  (',', ','),\n",
       "  ('if', 'IN'),\n",
       "  ('yes', 'UH'),\n",
       "  ('please', 'VB'),\n",
       "  ('New', 'NNP'),\n",
       "  ('York', 'NNP'),\n",
       "  ('it', 'PRP'),\n",
       "  ('.', '.')],\n",
       " [('Also', 'RB'),\n",
       "  ('consider', 'VBP'),\n",
       "  ('the', 'DT'),\n",
       "  ('value', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('City', 'NNP'),\n",
       "  ('when', 'WRB'),\n",
       "  ('talking', 'VBG'),\n",
       "  ('about', 'IN'),\n",
       "  ('Property', 'NNP'),\n",
       "  ('which', 'WDT'),\n",
       "  ('is', 'VBZ'),\n",
       "  ('worth', 'IN'),\n",
       "  ('more', 'JJR'),\n",
       "  ('than', 'IN'),\n",
       "  ('a', 'DT'),\n",
       "  ('Million', 'NNP'),\n",
       "  ('Dollars', 'NNP')]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ie_preprocess(docu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nltk.corpus.reader.util.ConcatenatedCorpusView'>\n",
      "[',', '.', 'accomplished', 'analytically', 'appear', 'apt', 'associated', 'assuming', 'became', 'become', 'been', 'began', 'call', 'called', 'carefully', 'chose', 'classified', 'colorful', 'composed', 'contain', 'differed', 'difficult', 'encountered', 'enough', 'equate', 'extremely', 'found', 'happens', 'have', 'ignored', 'in', 'involved', 'more', 'needed', 'nightly', 'observed', 'of', 'on', 'out', 'quite', 'represent', 'responsible', 'revamped', 'seclude', 'set', 'shortened', 'sing', 'sounded', 'stated', 'still', 'sung', 'supported', 'than', 'to', 'when', 'work']\n"
     ]
    }
   ],
   "source": [
    "# 2.8   Exploring Tagged Corpora\n",
    "# Finding words that APPEAR afer another Word \n",
    "\n",
    "from nltk.corpus import brown\n",
    "\n",
    "brown_learned_text = brown.words(categories='learned')\n",
    "#\n",
    "print(type(brown_learned_text))  # http://www.nltk.org/api/nltk.corpus.reader.html\n",
    "# <class 'nltk.corpus.reader.util.ConcatenatedCorpusView'>\n",
    "#\n",
    "print(sorted(set(b for (a, b) in nltk.bigrams(brown_learned_text) if a == 'often')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'words'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-730f3713184d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtext1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"we are often told that often is a word which will appear often in this text also after often shall occur many other words  \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtext2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategories\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'learned'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbigrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'often'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'words'"
     ]
    }
   ],
   "source": [
    "# Experi own -- Fail as we dont have a -- <class 'nltk.corpus.reader.util.ConcatenatedCorpusView'>\n",
    "\n",
    "text1 = \"we are often told that often is a word which will appear often in this text also after often shall occur many other words  \"\n",
    "\n",
    "text2 = text1.words(categories='learned')  ### AttributeError: 'str' object has no attribute 'words'\n",
    "sorted(set(b for (a, b) in nltk.bigrams(text1) if a == 'often'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-17-0dcf545f4eb6>, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-17-0dcf545f4eb6>\"\u001b[0;36m, line \u001b[0;32m12\u001b[0m\n\u001b[0;31m    for tagged_sent in brown.tagged_sents():\u001b[0m\n\u001b[0m                                            ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "#Next, let's look at some larger context, and find words involving particular sequences \n",
    "# of tags and words (in this case \"<Verb> to <Verb>\"). In code-three-word-phrase we consider\n",
    "# each three-word window in the sentence [1], and check if they meet our criterion [2]. \n",
    "# If the tags match, we print the corresponding words [3].\n",
    "\n",
    "from nltk.corpus import brown\n",
    "def process(sentence):\n",
    "    for (w1,t1), (w2,t2), (w3,t3) in nltk.trigrams(sentence):             ### [1]\n",
    "        if (t1.startswith('V') and t2 == 'TO' and t3.startswith('V')):    ### [2]\n",
    "            #print(w1, w2, w3)                ### [3] --- Works OK -- Dont Uncomment Large Dump .... #### DHANK \n",
    "\n",
    "for tagged_sent in brown.tagged_sents():\n",
    "    process(tagged_sent)\n",
    "    \n",
    "    # \n",
    "    '''\n",
    "    combined to achieve\n",
    "continue to place\n",
    "serve to protect\n",
    "wanted to wait\n",
    "allowed to place\n",
    "expected to become\n",
    "expected to approve\n",
    "expected to make\n",
    "intends to make\n",
    "seek to set\n",
    "like to see\n",
    "designed to provide\n",
    "get to hear\n",
    "expects to tell\n",
    "expected to give\n",
    "prefer to pay\n",
    "required to obtain\n",
    "permitted to teach\n",
    "designed to reduce\n",
    "Asked to elaborate\n",
    "\n",
    "'''\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exposure to group\n",
      "respite to hurry\n",
      "urgings to date\n"
     ]
    }
   ],
   "source": [
    "# Noun to Noun \n",
    "\n",
    "from nltk.corpus import brown\n",
    "def process(sentence):\n",
    "    for (w1,t1), (w2,t2), (w3,t3) in nltk.trigrams(sentence):             ### [1]\n",
    "        if (t1.startswith('NN') and t2 == 'TO' and t3.startswith('NN')):    ### [2]\n",
    "            print(w1, w2, w3)                ### [3] --- Works OK -- Dont Uncomment Large Dump .... #### DHANK \n",
    "\n",
    "for tagged_sent in brown.tagged_sents():\n",
    "    process(tagged_sent)\n",
    "    \n",
    "    #  3 Only ? \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  Once we start doing part-of-speech tagging, we will be creating programs \n",
    "# that assign a tag to a word, the tag which is most likely in a given context #\n",
    "## CONTEXT is Important \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "<class 'dict'>\n",
      "{'colorless': 'ADJ'}\n",
      "{'colorless': 'ADJ', 'sleep': 'V', 'ideas': 'N', 'furiously': 'ADV'}\n",
      "['colorless', 'sleep', 'ideas', 'furiously']\n",
      "<class 'list'>\n",
      "['colorless', 'furiously', 'ideas', 'sleep']\n",
      "['ideas']\n",
      "['furiously']\n",
      "['colorless', 'sleep', 'ideas', 'furiously']\n",
      "['ADJ', 'V', 'N', 'ADV']\n",
      "[('colorless', 'ADJ'), ('sleep', 'V'), ('ideas', 'N'), ('furiously', 'ADV')]\n",
      "[('colorless', 'ADJ'), ('sleep', ['N', 'V']), ('ideas', 'N'), ('furiously', 'ADV')]\n"
     ]
    }
   ],
   "source": [
    "# Py DICTS \n",
    "\n",
    "pos = {}\n",
    "print(pos)\n",
    "print(type(pos))            # <class 'dict'>\n",
    "pos['colorless'] = 'ADJ'\n",
    "print(pos)\n",
    "pos['ideas'] = 'N'\n",
    "pos['sleep'] = 'V'\n",
    "pos['furiously'] = 'ADV'\n",
    "print(pos)\n",
    "#\n",
    "print(list(pos))           # Dict to LIST \n",
    "pos_list = list(pos)\n",
    "print(type(pos_list))      # <class 'list'>\n",
    "print(sorted(pos))\n",
    "print([w for w in pos if w.endswith('as')])\n",
    "print([w for w in pos if w.startswith('fu')])\n",
    "# \n",
    "# dictionary methods keys(), values() and items() \n",
    "# \n",
    "print(list(pos.keys()))\n",
    "#\n",
    "print(list(pos.values()))\n",
    "#\n",
    "print(list(pos.items()))\n",
    "#\n",
    "pos['sleep'] = ['N', 'V'] # Sleep == Noun + Verb , here the ['N', 'V'] IS LIST ASSIGNED AS VALUES IN KEY VALUES for DICT\n",
    "#\n",
    "print(list(pos.items()))\n",
    "#In fact, this is what we saw in 4 for the CMU Pronouncing Dictionary, which stores multiple pronunciations for a single word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-994798c788d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ideas'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'blogs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'adventures'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'N'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "# We have seen above that DICT - VALUES can be LISTS like --  ['N', 'V']\n",
    "# BUT we cant have LISTS as KEY's . LISTS are MUTABLE ...\n",
    "# Note that dictionary keys must be immutable types, such as strings and tuples. \n",
    "# If we try to define a dictionary using a mutable key, we get a TypeError:\n",
    "\n",
    "\n",
    "pos = {['ideas', 'blogs', 'adventures']: 'N'}\n",
    "\n",
    "# http://stackoverflow.com/questions/6754102/typeerror-unhashable-type\n",
    "# http://stackoverflow.com/questions/6754102/typeerror-unhashable-type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TBD -- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TBD -- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TBD -- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TBD -- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TBD -- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NN'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4   Automatic Tagging\n",
    "#\n",
    "from nltk.corpus import brown\n",
    "brown_tagged_sents = brown.tagged_sents(categories='news')\n",
    "brown_sents = brown.sents(categories='news')\n",
    "# \n",
    "# We find the Most Frequent TAG is - NN ( NOUN) - noun, singular ... and Assign it as the DEFAULT TAG \n",
    "#\n",
    "tags = [tag for (word, tag) in brown.tagged_words(categories='news')]\n",
    "nltk.FreqDist(tags).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'NN'),\n",
       " ('do', 'NN'),\n",
       " ('not', 'NN'),\n",
       " ('like', 'NN'),\n",
       " ('green', 'NN'),\n",
       " ('eggs', 'NN'),\n",
       " ('and', 'NN'),\n",
       " ('ham', 'NN'),\n",
       " (',', 'NN'),\n",
       " ('I', 'NN'),\n",
       " ('do', 'NN'),\n",
       " ('not', 'NN'),\n",
       " ('like', 'NN'),\n",
       " ('them', 'NN'),\n",
       " ('Sam', 'NN'),\n",
       " ('I', 'NN'),\n",
       " ('am', 'NN'),\n",
       " ('!', 'NN')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "raw = 'I do not like green eggs and ham, I do not like them Sam I am!'\n",
    "tokens = word_tokenize(raw)\n",
    "default_tagger = nltk.DefaultTagger('NN')\n",
    "default_tagger.tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 4.2   The Regular Expression Tagger\n",
    "# \n",
    "# Note that these are processed in order, and the first one that matches is applied.\n",
    "#\n",
    "\n",
    "patterns = [\n",
    "    (r'.*ing$', 'VBG'),               # gerunds\n",
    "    (r'.*ed$', 'VBD'),                # simple past\n",
    "     (r'.*es$', 'VBZ'),                # 3rd singular present\n",
    "     (r'.*ould$', 'MD'),               # modals\n",
    "     (r'.*\\'s$', 'NN$'),               # possessive nouns\n",
    "     (r'.*s$', 'NNS'),                 # plural nouns\n",
    "     (r'^-?[0-9]+(.[0-9]+)?$', 'CD'),  # cardinal numbers\n",
    "     (r'.*', 'NN')                     # nouns (default)\n",
    " ]\n",
    "\n",
    "## DHANKAR--- \n",
    "# Seen above -- The final regular expression «.*» is a catch-all that tags everything as a noun.\n",
    "# Similary in ELIZA or any other BOT the Last Regex Search is a CATCH ALL -- after all other possibilities have been \n",
    "# explored --- we cant have the CATCH All come earlier as it will CATCH All and leave nothing for others . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20326391789486245"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regexp_tagger = nltk.RegexpTagger(patterns)\n",
    "regexp_tagger.tag(brown_sents[3])\n",
    "regexp_tagger.evaluate(brown_tagged_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45578495136941344"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4.3   The Lookup Tagger\n",
    "\n",
    "'''\n",
    "A lot of high-frequency words do not have the NN tag. Let's find the hundred most frequent words and store their most likely tag. \n",
    "We can then use this information as the model for a \"lookup tagger\" (an NLTK UnigramTagger):\n",
    "'''\n",
    "\n",
    "fd = nltk.FreqDist(brown.words(categories='news'))\n",
    "cfd = nltk.ConditionalFreqDist(brown.tagged_words(categories='news'))\n",
    "most_freq_words = fd.most_common(100)\n",
    "\n",
    "likely_tags = dict((word, cfd[word].max()) for (word, _) in most_freq_words)     ####  HOW ??? \n",
    "\n",
    "baseline_tagger = nltk.UnigramTagger(model=likely_tags)\n",
    "baseline_tagger.evaluate(brown_tagged_sents)      # Eval tagger against the BROWN Corpus - tagged Sentences \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def performance(cfd, wordlist):\n",
    "    lt = dict((word, cfd[word].max()) for word in wordlist)\n",
    "    baseline_tagger = nltk.UnigramTagger(model=lt, backoff=nltk.DefaultTagger('NN'))\n",
    "    return baseline_tagger.evaluate(brown.tagged_sents(categories='news'))\n",
    "\n",
    "def display():\n",
    "    import pylab\n",
    "    word_freqs = nltk.FreqDist(brown.words(categories='news')).most_common()\n",
    "    words_by_freq = [w for (w, _) in word_freqs]\n",
    "    cfd = nltk.ConditionalFreqDist(brown.tagged_words(categories='news'))\n",
    "    sizes = 2 ** pylab.arange(15)\n",
    "    perfs = [performance(cfd, words_by_freq[:size]) for size in sizes]\n",
    "    pylab.plot(sizes, perfs, '-bo')\n",
    "    pylab.title('Lookup Tagger Performance with Varying Model Size')\n",
    "    pylab.xlabel('Model Size')\n",
    "    pylab.ylabel('Performance')\n",
    "    pylab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
