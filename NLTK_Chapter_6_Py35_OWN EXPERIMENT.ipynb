{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "# 1.3   Document Classification\n",
    "#\n",
    "import nltk\n",
    "import random\n",
    "import re\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "              for category in movie_reviews.categories()\n",
    "              for fileid in movie_reviews.fileids(category)]\n",
    "random.shuffle(documents)\n",
    "\n",
    "# print(documents) --- DONT \n",
    "print(len(documents))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['balderdash', 'choose', 'puncturing', 'handsome', 'vassili', 'ghosts', 'supply', 'fong', 'threesomes', 'secured', 'tiredness', 'deviously', 'surfaces', 'parkes', 'judaism', 'bikini', 'versatility', 'carver', 'stifle', 'arrested']\n"
     ]
    }
   ],
   "source": [
    "all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words()) # All Words - Freq Dist created Converted to Lowercase \n",
    "word_features = list(all_words)[:20]  ### Original -- Get a list of the 2000 most frequent words -- OWN 20 \n",
    "print(word_features)                                                  ### OK dont \n",
    "\n",
    "\n",
    "'''\n",
    "The reason that we compute the set of all words in a document in ###[3], rather than just checking\n",
    "if word in document, is that checking whether a word occurs in a set is much faster than checking \n",
    "whether it occurs in a list\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "def document_features(document): \n",
    "    document_words = set(document) ### [3]\n",
    "    features = {}\n",
    "    for word in word_features:                                         ### List created above \n",
    "        features['contains({})'.format(word)] = (word in document_words) ### Check if Word in INPUT_DOCUMENT == to Word in LIST word_features\n",
    "    return features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nltk.corpus.reader.util.StreamBackedCorpusView'>\n",
      "\n",
      "['capsule', ':', 'the', 'best', 'place', 'to', 'start', ...]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# See what the Review File is like ? \n",
    "#\n",
    "test1 = movie_reviews.words(\"pos/cv957_8737.txt\")\n",
    "print(type(test1))   ### http://www.nltk.org/_modules/nltk/corpus/reader/util.html\n",
    "                     ### <class 'nltk.corpus.reader.util.StreamBackedCorpusView'>\n",
    "print(\"\")\n",
    "print(movie_reviews.words(\"pos/cv957_8737.txt\"))\n",
    "\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'contains(fong)': False, 'contains(parkes)': False, 'contains(choose)': False, 'contains(balderdash)': False, 'contains(bikini)': False, 'contains(supply)': False, 'contains(puncturing)': False, 'contains(ghosts)': False, 'contains(deviously)': False, 'contains(tiredness)': False, 'contains(stifle)': False, 'contains(surfaces)': False, 'contains(carver)': False, 'contains(threesomes)': False, 'contains(judaism)': False, 'contains(vassili)': False, 'contains(secured)': False, 'contains(handsome)': False, 'contains(arrested)': False, 'contains(versatility)': False}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(document_features(movie_reviews.words('pos/cv957_8737.txt'))) # OK dont Print \n",
    "\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Needs to be Understood --------- \n",
    "## The LIST ---\"documents\" was RANDOM Shuffled \n",
    "\n",
    "featuresets = [(document_features(d), c) for (d,c) in documents]  ### \"documents\" is LIST from 1st Code Cell Above \n",
    "train_set, test_set = featuresets[100:], featuresets[:100] \n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set) ### Thus we wont have the same Document as above == 'pos/cv957_8737.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.52\n",
      "\n",
      "Most Informative Features\n",
      "        contains(bikini) = True              neg : pos    =      2.7 : 1.0\n",
      "        contains(ghosts) = True              pos : neg    =      1.7 : 1.0\n",
      "        contains(stifle) = True              neg : pos    =      1.7 : 1.0\n",
      "      contains(handsome) = True              pos : neg    =      1.5 : 1.0\n",
      "        contains(choose) = True              pos : neg    =      1.3 : 1.0\n"
     ]
    }
   ],
   "source": [
    "print(nltk.classify.accuracy(classifier, test_set)) ### [1]\n",
    "\n",
    "print(\"\")\n",
    "classifier.show_most_informative_features(5)\n",
    "\n",
    "# A review that Mentions -- neptune --- is 2.3 Times more likely to be POSITIVE than NEGATIVE !! \n",
    "### These values seen below will always be different as we have Random Shuffled Reviews ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nltk.probability.FreqDist'>\n"
     ]
    }
   ],
   "source": [
    "# 1.4   Part-of-Speech Tagging -- \n",
    "\n",
    "# Code Cell - Cross Checked from ALPHA Tutorial and Modified according best fit ...\n",
    "\n",
    "from nltk.corpus import brown\n",
    "\n",
    "suffix_fdist = nltk.FreqDist()\n",
    "print(type(suffix_fdist))              ## - <class 'nltk.probability.FreqDist'>\n",
    "\n",
    "for word in brown.words():             ### Original -- brown.words()\n",
    "#for word in movie_reviews.words():    ### Own -- movie_reviews.words()\n",
    "     word = word.lower()\n",
    "     suffix_fdist[word[-1:]] += 1      ### PDF Book Code == suffix_fdist.inc(word[-1:]) # .inc = Increment....\n",
    "     suffix_fdist[word[-2:]] += 1\n",
    "     suffix_fdist[word[-3:]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['e', ',', '.', 's', 'd', 't', 'he', 'n', 'a', 'of', 'the', 'y', 'r', 'to', 'in', 'f', 'o', 'ed', 'nd', 'is', 'on', 'l', 'g', 'and', 'ng', 'er', 'as', 'ing', 'h', 'at', 'es', 'or', 're', 'it', '``', 'an', \"''\", 'm', ';', 'i', 'ly', 'ion', 'en', 'al', '?', 'nt', 'be', 'hat', 'st', 'his', 'th', 'll', 'le', 'ce', 'by', 'ts', 'me', 've', \"'\", 'se', 'ut', 'was', 'for', 'ent', 'ch', 'k', 'w', 'ld', '`', 'rs', 'ted', 'ere', 'her', 'ne', 'ns', 'ith', 'ad', 'ry', ')', '(', 'te', '--', 'ay', 'ty', 'ot', 'p', 'nce', \"'s\", 'ter', 'om', 'ss', ':', 'we', 'are', 'c', 'ers', 'uld', 'had', 'so', 'ey']\n"
     ]
    }
   ],
   "source": [
    "common_suffixes = [suffix for (suffix, count) in suffix_fdist.most_common(100)] # Online Code NLTK.org \n",
    "\n",
    "#common_suffixes = suffix_fdist.keys()[:100] ### PDF Book Code - shorter -- Error \n",
    "print(common_suffixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['e', ',', 's', '.', 't', 'a', 'n', 'd', 'he', \"'\", 'y', 'the', 'r', 'is', 'of', 'to', 'o', '\"', 'g', 'in', '-', 'ng', 'nd', 'ing', 'f', 'er', 'l', 'and', ')', 'on', 'it', '(', 'h', 'as', 'es', 'at', 'ed', 're', 'i', 'm', 'ly', 'an', 'or', 'en', 'hat', 'his', 'st', 'ut', 'le', 'll', 'th', 'ne', 've', 'ts', 'k', 'me', 'al', 'be', 'ce', 'by', 'se', 'rs', 'nt', 'her', '?', 'ion', 'ith', 'w', 'ry', 'ch', 'ter', 'p', 'for', 'ot', 'lm', 'ilm', 'so', 'one', ':', 'but', 'ld', 'ie', 'ere', 'out', 'are', 'lly', 'ey', 'ke', 'ns', 'ers', 'ent', 'ay', 'nce', 'te', 'up', 'c', 'us', 'om', 'ow', 'et']\n"
     ]
    }
   ],
   "source": [
    "# Own with -- #for word in movie_reviews.words()\n",
    "\n",
    "\n",
    "common_suffixes1 = [suffix for (suffix, count) in suffix_fdist1.most_common(100)]\n",
    "print(common_suffixes1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## This is the FIRST Definition of POS Features Func \n",
    "\n",
    "def pos_features(word):\n",
    "     features = {}\n",
    "     for suffix in common_suffixes:\n",
    "         features['endswith({})'.format(suffix)] = word.lower().endswith(suffix)\n",
    "     return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"endswith('')\": False,\n",
       " \"endswith(')\": False,\n",
       " \"endswith('s)\": False,\n",
       " 'endswith(()': False,\n",
       " 'endswith())': False,\n",
       " 'endswith(,)': False,\n",
       " 'endswith(--)': False,\n",
       " 'endswith(.)': False,\n",
       " 'endswith(:)': False,\n",
       " 'endswith(;)': False,\n",
       " 'endswith(?)': False,\n",
       " 'endswith(`)': False,\n",
       " 'endswith(``)': False,\n",
       " 'endswith(a)': False,\n",
       " 'endswith(ad)': False,\n",
       " 'endswith(al)': False,\n",
       " 'endswith(an)': False,\n",
       " 'endswith(and)': False,\n",
       " 'endswith(are)': False,\n",
       " 'endswith(as)': False,\n",
       " 'endswith(at)': False,\n",
       " 'endswith(ay)': False,\n",
       " 'endswith(be)': False,\n",
       " 'endswith(by)': False,\n",
       " 'endswith(c)': False,\n",
       " 'endswith(ce)': False,\n",
       " 'endswith(ch)': False,\n",
       " 'endswith(d)': False,\n",
       " 'endswith(e)': False,\n",
       " 'endswith(ed)': False,\n",
       " 'endswith(en)': False,\n",
       " 'endswith(ent)': False,\n",
       " 'endswith(er)': False,\n",
       " 'endswith(ere)': False,\n",
       " 'endswith(ers)': False,\n",
       " 'endswith(es)': False,\n",
       " 'endswith(ey)': False,\n",
       " 'endswith(f)': False,\n",
       " 'endswith(for)': False,\n",
       " 'endswith(g)': False,\n",
       " 'endswith(h)': False,\n",
       " 'endswith(had)': False,\n",
       " 'endswith(hat)': False,\n",
       " 'endswith(he)': False,\n",
       " 'endswith(her)': False,\n",
       " 'endswith(his)': False,\n",
       " 'endswith(i)': False,\n",
       " 'endswith(in)': False,\n",
       " 'endswith(ing)': False,\n",
       " 'endswith(ion)': False,\n",
       " 'endswith(is)': False,\n",
       " 'endswith(it)': False,\n",
       " 'endswith(ith)': False,\n",
       " 'endswith(k)': False,\n",
       " 'endswith(l)': False,\n",
       " 'endswith(ld)': False,\n",
       " 'endswith(le)': False,\n",
       " 'endswith(ll)': False,\n",
       " 'endswith(ly)': True,\n",
       " 'endswith(m)': False,\n",
       " 'endswith(me)': False,\n",
       " 'endswith(n)': False,\n",
       " 'endswith(nce)': False,\n",
       " 'endswith(nd)': False,\n",
       " 'endswith(ne)': False,\n",
       " 'endswith(ng)': False,\n",
       " 'endswith(ns)': False,\n",
       " 'endswith(nt)': False,\n",
       " 'endswith(o)': False,\n",
       " 'endswith(of)': False,\n",
       " 'endswith(om)': False,\n",
       " 'endswith(on)': False,\n",
       " 'endswith(or)': False,\n",
       " 'endswith(ot)': False,\n",
       " 'endswith(p)': False,\n",
       " 'endswith(r)': False,\n",
       " 'endswith(re)': False,\n",
       " 'endswith(rs)': False,\n",
       " 'endswith(ry)': False,\n",
       " 'endswith(s)': False,\n",
       " 'endswith(se)': False,\n",
       " 'endswith(so)': False,\n",
       " 'endswith(ss)': False,\n",
       " 'endswith(st)': False,\n",
       " 'endswith(t)': False,\n",
       " 'endswith(te)': False,\n",
       " 'endswith(ted)': False,\n",
       " 'endswith(ter)': False,\n",
       " 'endswith(th)': False,\n",
       " 'endswith(the)': False,\n",
       " 'endswith(to)': False,\n",
       " 'endswith(ts)': False,\n",
       " 'endswith(ty)': False,\n",
       " 'endswith(uld)': False,\n",
       " 'endswith(ut)': False,\n",
       " 'endswith(ve)': False,\n",
       " 'endswith(w)': False,\n",
       " 'endswith(was)': False,\n",
       " 'endswith(we)': False,\n",
       " 'endswith(y)': True}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing - pos_features(word)\n",
    "\n",
    "pos_features(\"lovely\")\n",
    "\n",
    "# As seen below -- 'endswith(ly)': True, -----AND----- 'endswith(y)': True, \n",
    "# The --- endswith--- SUFFIXes - ly and y are VALUED : TRUE ...\n",
    "# Page -1 of ALPHA PDF \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### IMPORTANT from here Onwards \n",
    "\n",
    "# 1.5   Exploiting Context\n",
    "#### ALPHA - PDF Page 2 --- Starts with this Before Classifier \n",
    "\n",
    "## This is the IInd Definition of POS Features Func \n",
    "\n",
    "import nltk\n",
    "import random\n",
    "import re\n",
    "#from nltk.corpus import movie_reviews\n",
    "from nltk.corpus import brown\n",
    "\n",
    "\n",
    "def pos_features(sentence, i): ###[1]\n",
    "    features = {\"suffix(1)\": sentence[i][-1:],\n",
    "                \"suffix(2)\": sentence[i][-2:],\n",
    "                \"suffix(3)\": sentence[i][-3:],   ### Original Ends here with --- \"suffix(3)\"\n",
    "               \"suffix(4)\": sentence[i][-4:], \n",
    "               \"suffix(5)\": sentence[i][-5:],\n",
    "               \"suffix(6)\": sentence[i][-6:]}    ### No COMMA for the Last --- sentence[i]\n",
    "    if i == 0:\n",
    "        features[\"prev-word\"] = \"<START>\"\n",
    "    else:\n",
    "        features[\"prev-word\"] = sentence[i-1]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.']\n",
      "__________________________________________________\n",
      " \n",
      "['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.']\n",
      "__________________________________________________\n",
      " \n",
      "['The', 'September-October', 'term', 'jury', 'had', 'been', 'charged', 'by', 'Fulton', 'Superior', 'Court', 'Judge', 'Durwood', 'Pye', 'to', 'investigate', 'reports', 'of', 'possible', '``', 'irregularities', \"''\", 'in', 'the', 'hard-fought', 'primary', 'which', 'was', 'won', 'by', 'Mayor-nominate', 'Ivan', 'Allen', 'Jr.', '.']\n",
      "__________________________________________________\n",
      " \n",
      "['``', 'Only', 'a', 'relative', 'handful', 'of', 'such', 'reports', 'was', 'received', \"''\", ',', 'the', 'jury', 'said', ',', '``', 'considering', 'the', 'widespread', 'interest', 'in', 'the', 'election', ',', 'the', 'number', 'of', 'voters', 'and', 'the', 'size', 'of', 'this', 'city', \"''\", '.']\n",
      "__________________________________________________\n",
      " \n"
     ]
    }
   ],
   "source": [
    "print(brown.sents()[0]) # Own Code --- not done in book ? \n",
    "print(\"_\"*50)\n",
    "print(\" \")\n",
    "\n",
    "print(brown.sents()[1]) \n",
    "print(\"_\"*50)\n",
    "print(\" \")\n",
    "\n",
    "print(brown.sents()[2]) \n",
    "print(\"_\"*50)\n",
    "print(\" \")\n",
    "\n",
    "print(brown.sents()[3])   ### Three Instances of \"of\" \n",
    "print(\"_\"*50)\n",
    "print(\" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prev-word': 'of',\n",
       " 'suffix(1)': 'h',\n",
       " 'suffix(2)': 'ch',\n",
       " 'suffix(3)': 'uch',\n",
       " 'suffix(4)': 'such',\n",
       " 'suffix(5)': 'such',\n",
       " 'suffix(6)': 'such'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_features(brown.sents()[3], 6)  ## 3rd Sentence --- 6th Word [i]\n",
    "\n",
    "# Actually no point as - all three instances of \"of\" shall have diff index values [i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prev-word': \"Atlanta's\",\n",
       " 'suffix(1)': 't',\n",
       " 'suffix(2)': 'nt',\n",
       " 'suffix(3)': 'ent'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_features(brown.sents()[0], 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prev-word': 'evidence',\n",
       " 'suffix(1)': \"'\",\n",
       " 'suffix(2)': \"''\",\n",
       " 'suffix(3)': \"''\"}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_features(brown.sents()[0], 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prev-word': 'Jury',\n",
       " 'suffix(1)': 'd',\n",
       " 'suffix(2)': 'id',\n",
       " 'suffix(3)': 'aid',\n",
       " 'suffix(4)': 'said',\n",
       " 'suffix(5)': 'said',\n",
       " 'suffix(6)': 'said'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_features(brown.sents()[0], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1.5   Exploiting Context -- Original Book Code \n",
    "### POS_Features with CONTEXT only - 3 Sufixes \n",
    "\n",
    "import nltk\n",
    "import random\n",
    "import re\n",
    "#from nltk.corpus import movie_reviews\n",
    "from nltk.corpus import brown\n",
    "\n",
    "\n",
    "def pos_features1(sentence, i): ###[1]\n",
    "    features = {\"suffix(1)\": sentence[i][-1:],\n",
    "                \"suffix(2)\": sentence[i][-2:],\n",
    "                \"suffix(3)\": sentence[i][-3:]   ### Original Ends here with --- \"suffix(3)\"\n",
    "               } ## No COMMA for the Last --- sentence[i]\n",
    "    if i == 0:\n",
    "        features[\"prev-word\"] = \"<START>\"   ### If i=Index of WORD is ZERO- its \"FirstWord\" then-\"prev-word\"=\"<START>\"  \n",
    "    else:\n",
    "        features[\"prev-word\"] = sentence[i-1]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[({'prev-word': '<START>', 'suffix(1)': 'e', 'suffix(2)': 'he', 'suffix(3)': 'The'}, 'AT'), ({'prev-word': 'The', 'suffix(1)': 'n', 'suffix(2)': 'on', 'suffix(3)': 'ton'}, 'NP-TL'), ({'prev-word': 'Fulton', 'suffix(1)': 'y', 'suffix(2)': 'ty', 'suffix(3)': 'nty'}, 'NN-TL'), ({'prev-word': 'County', 'suffix(1)': 'd', 'suffix(2)': 'nd', 'suffix(3)': 'and'}, 'JJ-TL'), ({'prev-word': 'Grand', 'suffix(1)': 'y', 'suffix(2)': 'ry', 'suffix(3)': 'ury'}, 'NN-TL')]\n",
      "10055\n"
     ]
    }
   ],
   "source": [
    "\"\"\"    \n",
    "Now we take all the sentences in the --news--- portion of Brown and apply our function to\n",
    "get the POS features, as a dictionary, of each (untagged) word. Then we pair that\n",
    "with the tag to get feature sets.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "tagged_sents = brown.tagged_sents(categories='news')\n",
    "featuresets = []\n",
    "for tagged_sent in tagged_sents:\n",
    "     untagged_sent = nltk.tag.untag(tagged_sent)\n",
    "     for i, (word, tag) in enumerate(tagged_sent):\n",
    "         featuresets.append( (pos_features1(untagged_sent, i), tag) )   ### pos_features1 -- Original only 3 SUFFIX's \n",
    "\n",
    "small_ftr_set = featuresets[:5]\n",
    "print(small_ftr_set)\n",
    "size = int(len(featuresets) * 0.1)\n",
    "print(size) ## DHANK ## 10055\n",
    "\n",
    "# ['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of',...... The Sentence \n",
    "# \n",
    "# ({'suffix(1)': 'e', 'prev-word': '<START>', 'suffix(2)': 'he', 'suffix(3)': 'The'}, 'AT'),........Console output - seen below.  \n",
    "#\n",
    "# 'prev-word': '<START>' === As there is NO Previous Word - this is the First Words Suffix -1 \n",
    "#\n",
    "#  'suffix(1)': 'e' --- Last Letter of the SUFFIX word if the Word Has Only Three Letters \n",
    "#\n",
    "# From For Loop above --- 'AT' --- the TAG of the Word ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7891596220785678\n"
     ]
    }
   ],
   "source": [
    "## NaiveBayesClassifier\n",
    "\n",
    "train_set, test_set = featuresets[size:], featuresets[:size]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))\n",
    "\n",
    "##  EARLIER --- 0.8654400795624068\n",
    "## NOW --- 0.7891596220785678  ##--------Same as NLTK PDF Book \n",
    "\n",
    "#classifier.classify(pos_features('cats'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " def pos_features(sentence, i, history):                            ### [1]\n",
    "     features = {\"suffix(1)\": sentence[i][-1:],\n",
    "                 \"suffix(2)\": sentence[i][-2:],\n",
    "                 \"suffix(3)\": sentence[i][-3:]}\n",
    "     if i == 0:\n",
    "         features[\"prev-word\"] = \"<START>\"\n",
    "         features[\"prev-tag\"] = \"<START>\"\n",
    "     else:\n",
    "         features[\"prev-word\"] = sentence[i-1]\n",
    "         features[\"prev-tag\"] = history[i-1]\n",
    "     return features\n",
    "\n",
    "class ConsecutivePosTagger(nltk.TaggerI):                           ### [2]\n",
    "\n",
    "    def __init__(self, train_sents):\n",
    "        train_set = []\n",
    "        for tagged_sent in train_sents:\n",
    "            untagged_sent = nltk.tag.untag(tagged_sent)\n",
    "            history = []\n",
    "            for i, (word, tag) in enumerate(tagged_sent):\n",
    "                featureset = pos_features(untagged_sent, i, history)\n",
    "                train_set.append( (featureset, tag) )\n",
    "                history.append(tag)\n",
    "        self.classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "    def tag(self, sentence):\n",
    "        history = []\n",
    "        for i, word in enumerate(sentence):\n",
    "            featureset = pos_features(sentence, i, history)\n",
    "            tag = self.classifier.classify(featureset)\n",
    "            history.append(tag)\n",
    "        return zip(sentence, history)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7980528511821975\n"
     ]
    }
   ],
   "source": [
    "tagged_sents = brown.tagged_sents(categories='news')\n",
    "size = int(len(tagged_sents) * 0.1)\n",
    "train_sents, test_sents = tagged_sents[size:], tagged_sents[:size]\n",
    "tagger = ConsecutivePosTagger(train_sents)\n",
    "print(tagger.evaluate(test_sents))\n",
    "\n",
    "# Own -- 0.79805\n",
    "\n",
    "# NLTK.org - PDF  - 0.79796012981\n",
    "\n",
    "\"\"\"\n",
    "IMPORTANT READ --- Page 233\n",
    "\n",
    "Other Methods for Sequence Classification -- \n",
    "\n",
    "1/ Hidden Markov Models \n",
    "2/ Maximum Entropy Markov Models\n",
    "3/ Linear-Chain Conditional Random Field Models\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sents = nltk.corpus.treebank_raw.sents()\n",
    "tokens = []\n",
    "boundaries = set()\n",
    "offset = 0\n",
    "for sent in sents:\n",
    "     tokens.extend(sent)\n",
    "     offset += len(sent)\n",
    "     boundaries.add(offset-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def punct_features(tokens, i):\n",
    "     return {'next-word-capitalized': tokens[i+1][0].isupper(),\n",
    "             'prev-word': tokens[i-1].lower(),\n",
    "             'punct': tokens[i],\n",
    "             'prev-word-is-one-char': len(tokens[i-1]) == 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Sfeaturesets = [(punct_features(tokens, i), (i in boundaries))\n",
    "                for i in range(1, len(tokens)-1)               ### RANGE = 1 to Length Tokens -1 \n",
    "                if tokens[i] in '.?!']                         ### if Token at Index - i == \".?!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.936026936026936"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size = int(len(Sfeaturesets) * 0.1)\n",
    "Strain_set, Stest_set = Sfeaturesets[size:], Sfeaturesets[:size]\n",
    "Sclassifier = nltk.NaiveBayesClassifier.train(Strain_set)\n",
    "nltk.classify.accuracy(Sclassifier, Stest_set)\n",
    "\n",
    "# Own -- 0.936026936026936"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def segment_sentences(words):\n",
    "    start = 0\n",
    "    sents = []\n",
    "    for i, word in enumerate(words):\n",
    "        if word in '.?!' and classifier.classify(punct_features(words, i)) == True:\n",
    "            sents.append(words[start:i+1])\n",
    "            start = i+1\n",
    "    if start < len(words):\n",
    "        sents.append(words[start:])\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101797\n",
      "[['.'], ['START', 'Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'Nov', '.', '29', '.', 'Mr', '.', 'Vinken', 'is', 'chairman', 'of', 'Elsevier', 'N', '.', 'V', '.,', 'the', 'Dutch', 'publishing', 'group', '.'], ['.', 'START', 'Rudolph', 'Agnew', ',', '55', 'years', 'old', 'and', 'former', 'chairman', 'of', 'Consolidated', 'Gold', 'Fields', 'PLC', ',', 'was', 'named', 'a', 'nonexecutive', 'director', 'of', 'this', 'British', 'industrial', 'conglomerate', '.', '.', 'START', 'A', 'form', 'of', 'asbestos', 'once', 'used', 'to', 'make', 'Kent', 'cigarette', 'filters', 'has', 'caused', 'a', 'high', 'percentage', 'of', 'cancer', 'deaths', 'among', 'a', 'group', 'of', 'workers', 'exposed', 'to', 'it', 'more', 'than', '30', 'years', 'ago', ',', 'researchers', 'reported', '.'], ['The', 'asbestos', 'fiber', ',', 'crocidolite', ',', 'is', 'unusually', 'resilient', 'once', 'it', 'enters', 'the', 'lungs', ',', 'with', 'even', 'brief', 'exposures', 'to', 'it', 'causing', 'symptoms', 'that', 'show', 'up', 'decades', 'later', ',', 'researchers', 'said', '.'], ['Lorillard', 'Inc', '.,', 'the', 'unit', 'of', 'New', 'York', '-', 'based', 'Loews', 'Corp', '.', 'that', 'makes', 'Kent', 'cigarettes', ',', 'stopped', 'using', 'crocidolite', 'in', 'its', 'Micronite', 'cigarette', 'filters', 'in', '1956', '.'], ['Although', 'preliminary', 'findings', 'were', 'reported', 'more', 'than', 'a', 'year', 'ago', ',', 'the', 'latest', 'results', 'appear', 'in', 'today', \"'\", 's', 'New', 'England', 'Journal', 'of', 'Medicine', ',', 'a', 'forum', 'likely', 'to', 'bring', 'new', 'attention', 'to', 'the', 'problem', '.'], ['A', 'Lorillard', 'spokewoman', 'said', ',', '\"', 'This', 'is', 'an', 'old', 'story', '.'], ['We', \"'\", 're', 'talking', 'about', 'years', 'ago', 'before', 'anyone', 'heard', 'of', 'asbestos', 'having', 'any', 'questionable', 'properties', '.'], ['There', 'is', 'no', 'asbestos', 'in', 'our', 'products', 'now', '.\"', 'Neither', 'Lorillard', 'nor', 'the', 'researchers', 'who', 'studied', 'the', 'workers', 'were', 'aware', 'of', 'any', 'research', 'on', 'smokers', 'of', 'the', 'Kent', 'cigarettes', '.'], ['\"', 'We', 'have', 'no', 'useful', 'information', 'on', 'whether', 'users', 'are', 'at', 'risk', ',\"', 'said', 'James', 'A', '.', 'Talcott', 'of', 'Boston', \"'\", 's', 'Dana', '-', 'Farber', 'Cancer', 'Institute', '.'], ['Dr', '.', 'Talcott', 'led', 'a', 'team', 'of', 'researchers', 'from', 'the', 'National', 'Cancer', 'Institute', 'and', 'the', 'medical', 'schools', 'of', 'Harvard', 'University', 'and', 'Boston', 'University', '.'], ['The', 'Lorillard', 'spokeswoman', 'said', 'asbestos', 'was', 'used', 'in', '\"', 'very', 'modest', 'amounts', '\"', 'in', 'making', 'paper', 'for', 'the', 'filters', 'in', 'the', 'early', '1950s', 'and', 'replaced', 'with', 'a', 'different', 'type', 'of', 'filter', 'in', '1956', '.'], ['From', '1953', 'to', '1955', ',', '9', '.', '8', 'billion', 'Kent', 'cigarettes', 'with', 'the', 'filters', 'were', 'sold', ',', 'the', 'company', 'said', '.'], ['Among', '33', 'men', 'who', 'worked', 'closely', 'with', 'the', 'substance', ',', '28', 'have', 'died', '--', 'more', 'than', 'three', 'times', 'the', 'expected', 'number', '.'], ['Four', 'of', 'the', 'five', 'surviving', 'workers', 'have', 'asbestos', '-', 'related', 'diseases', ',', 'including', 'three', 'with', 'recently', 'diagnosed', 'cancer', '.'], ['The', 'total', 'of', '18', 'deaths', 'from', 'malignant', 'mesothelioma', ',', 'lung', 'cancer', 'and', 'asbestosis', 'was', 'far', 'higher', 'than', 'expected', ',', 'the', 'researchers', 'said', '.'], ['\"', 'The', 'morbidity', 'rate', 'is', 'a', 'striking', 'finding', 'among', 'those', 'of', 'us', 'who', 'study', 'asbestos', '-', 'related', 'diseases', ',\"', 'said', 'Dr', '.', 'Talcott', '.'], ['The', 'percentage', 'of', 'lung', 'cancer', 'deaths', 'among', 'the', 'workers', 'at', 'the', 'West', 'Groton', ',', 'Mass', '.,', 'paper', 'factory', 'appears', 'to', 'be', 'the', 'highest', 'for', 'any', 'asbestos', 'workers', 'studied', 'in', 'Western', 'industrialized', 'countries', ',', 'he', 'said', '.'], ['The', 'plant', ',', 'which', 'is', 'owned', 'by', 'Hollingsworth', '&', 'Vose']]\n"
     ]
    }
   ],
   "source": [
    "# We had created an Empty List - tokens - which has all the End of Sntence Punctuation Marks \n",
    "# Lets use a Slice of this List to test the Func - segment_sentences(words)\n",
    "\n",
    "print(len(tokens))\n",
    "\n",
    "test_tokens = tokens[:500]\n",
    "\n",
    "print(segment_sentences(test_tokens))\n",
    "\n",
    "## Seen below - FIRST Letter /Alphabet - of every WORD that comes after Punctuation mark has been Capitalized \n",
    "## Whats with the Capitalised START --- needs to be checked further !!!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.668\n"
     ]
    }
   ],
   "source": [
    "### Dialogue \n",
    "\n",
    "posts = nltk.corpus.nps_chat.xml_posts()[:10000]\n",
    "\n",
    "\"\"\"Next, we'll define a simple feature extractor that checks what words the post contains: \"\"\"\n",
    "  \n",
    "def dialogue_act_features(post):\n",
    "     features = {}\n",
    "     for word in nltk.word_tokenize(post):\n",
    "         features['contains({})'.format(word.lower())] = True\n",
    "     return features\n",
    "\n",
    "\"\"\"\n",
    "Finally, we construct the training and testing data by applying the feature extractor to each post \n",
    "(using post.get('class') to get a post's dialogue act type), and create a new classifier:\n",
    "\n",
    "\"\"\"\n",
    "featuresets = [(dialogue_act_features(post.text), post.get('class'))\n",
    "                for post in posts]\n",
    "size = int(len(featuresets) * 0.1)\n",
    "train_set, test_set = featuresets[size:], featuresets[:size]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 apple\n",
      "1 banana\n",
      "2 grapes\n",
      "3 pear\n"
     ]
    }
   ],
   "source": [
    "# Sand-Box# Func Enumerate == Source - http://book.pythontips.com/en/latest/enumerate.html\n",
    "\n",
    "my_list = ['apple', 'banana', 'grapes', 'pear']\n",
    "for c, value in enumerate(my_list, 0):               ### This Optional Argument  == 0 , will Tell Enumerate to start \n",
    "                                                     ### the Index with Number 0 , thus 0 Apple \n",
    "                                                     ## change it to == 1 , 1 Apple , 2 Banana etc ... \n",
    "    print(c, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Where does this Code Cell Fit in ???\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tagged_words = brown.tagged_words(categories='news')\n",
    "featuresets = [(pos_features(n), g) for (n,g) in tagged_words]\n",
    "\n",
    "#print(len(featuresets))  ### Dont Print --- 100554\n",
    "#\n",
    "size = int(len(featuresets) * 0.1)\n",
    "train_set, test_set = featuresets[size:], featuresets[:size]\n",
    "\n",
    "# \n",
    "\n",
    "classifier = nltk.DecisionTreeClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)\n",
    "\n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### HAVASI MAIN CODE SOURCE --- http://web.media.mit.edu/~havasi/MAS.S60/code/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
