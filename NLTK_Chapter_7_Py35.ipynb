{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Source - Chapter - 7 \n",
    "\n",
    "import nltk, re, pprint\n",
    "\n",
    "docu = \" Hi there is there an entity in here , if yes please New York it. Also consider the value of the  City when talking about Property which is worth more than a Million Dollars \"\n",
    "\n",
    "def ie_preprocess(docu):\n",
    "    sentences = nltk.sent_tokenize(docu)\n",
    "    sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "    sentences1 = [nltk.pos_tag(sent) for sent in sentences]\n",
    "    \n",
    "    return sentences1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Hi', 'NNP'),\n",
       "  ('there', 'EX'),\n",
       "  ('is', 'VBZ'),\n",
       "  ('there', 'EX'),\n",
       "  ('an', 'DT'),\n",
       "  ('entity', 'NN'),\n",
       "  ('in', 'IN'),\n",
       "  ('here', 'RB'),\n",
       "  (',', ','),\n",
       "  ('if', 'IN'),\n",
       "  ('yes', 'UH'),\n",
       "  ('please', 'VB'),\n",
       "  ('New', 'NNP'),\n",
       "  ('York', 'NNP'),\n",
       "  ('it', 'PRP'),\n",
       "  ('.', '.')],\n",
       " [('Also', 'RB'),\n",
       "  ('consider', 'VBP'),\n",
       "  ('the', 'DT'),\n",
       "  ('value', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('City', 'NNP'),\n",
       "  ('when', 'WRB'),\n",
       "  ('talking', 'VBG'),\n",
       "  ('about', 'IN'),\n",
       "  ('Property', 'NNP'),\n",
       "  ('which', 'WDT'),\n",
       "  ('is', 'VBZ'),\n",
       "  ('worth', 'IN'),\n",
       "  ('more', 'JJR'),\n",
       "  ('than', 'IN'),\n",
       "  ('a', 'DT'),\n",
       "  ('Million', 'NNP'),\n",
       "  ('Dollars', 'NNP')]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \n",
    "\n",
    "ie_preprocess(docu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP the/DT little/JJ yellow/JJ dog/NN)\n",
      "  barked/VBD\n",
      "  at/IN\n",
      "  (NP the/DT cat/NN))\n"
     ]
    }
   ],
   "source": [
    "# NP Chunking \n",
    "# Also see -- https://ifarm.nl/erikt/research/np-chunking.html\n",
    "#\n",
    "\n",
    "sentence = [(\"the\", \"DT\"), (\"little\", \"JJ\"), (\"yellow\", \"JJ\"), (\"dog\", \"NN\"), (\"barked\", \"VBD\"), (\"at\", \"IN\"),(\"the\", \"DT\"), (\"cat\", \"NN\")]\n",
    "\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\" \n",
    "\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "result = cp.parse(sentence)\n",
    "print(result)\n",
    "\n",
    "result.draw() # All OK - Pop Up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP money/NN market/NN) fund/NN)\n"
     ]
    }
   ],
   "source": [
    "#  2.3   Chunking with Regular Expressions\n",
    "\n",
    "'''\n",
    "If a tag pattern matches at overlapping locations, the leftmost match takes precedence. \n",
    "For example, if we apply a rule that matches two consecutive nouns to a text \n",
    "containing three consecutive nouns, then only the first two nouns will be chunked:\n",
    "'''\n",
    "\n",
    "\n",
    "nouns = [(\"money\", \"NN\"), (\"market\", \"NN\"), (\"fund\", \"NN\")]\n",
    "\n",
    "# Seen above in nouns - all three Tokens have been Tagged as NOUNS --- the grammar below states -- \n",
    "# Chunk together 2 Consecutive Nouns -- the Left pair shall be Chunked not the RIGHT Pair .. \n",
    "\n",
    "grammar = \"NP: {<NN><NN>}  # Chunk two consecutive nouns\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "result =cp.parse(nouns)\n",
    "print(result)\n",
    "\n",
    "result.draw() # All OK - Pop Up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(CHUNK exposure/NN to/TO group/NN)\n",
      "(CHUNK respite/NN to/TO hurry/NN)\n",
      "(CHUNK urgings/NNS to/TO date/NN)\n"
     ]
    }
   ],
   "source": [
    "# 2.4   Exploring Text Corpora\n",
    "# \n",
    "# In Chapter-5 we saw how we could interrogate a tagged corpus to extract phrases matching a particular sequence\n",
    "# of part-of-speech tags. We can do the same work more easily with a chunker, as follows:\n",
    "\n",
    "#cp = nltk.RegexpParser('CHUNK: {<V.*> <TO> <V.*>}')\n",
    "cp = nltk.RegexpParser('CHUNK: {<NN.*> <TO> <NN.*>}')\n",
    "brown = nltk.corpus.brown\n",
    "for sent in brown.tagged_sents():\n",
    "     tree = cp.parse(sent)\n",
    "     for subtree in tree.subtrees():\n",
    "         if subtree.label() == 'CHUNK': \n",
    "                print(subtree)   #### Dhankar --- Dont Print Large Dump \n",
    "                \n",
    "                '''\n",
    "                \n",
    "                #WITH --- #cp = nltk.RegexpParser('CHUNK: {<V.*> <TO> <V.*>}')\n",
    "                \n",
    "                \n",
    "                (CHUNK combined/VBN to/TO achieve/VB)\n",
    "(CHUNK continue/VB to/TO place/VB)\n",
    "(CHUNK serve/VB to/TO protect/VB)\n",
    "(CHUNK wanted/VBD to/TO wait/VB)\n",
    "(CHUNK allowed/VBN to/TO place/VB)\n",
    "(CHUNK expected/VBN to/TO become/VB)\n",
    "(CHUNK expected/VBN to/TO approve/VB)\n",
    "(CHUNK expected/VBN to/TO make/VB)\n",
    "(CHUNK intends/VBZ to/TO make/VB)\n",
    "(CHUNK seek/VB to/TO set/VB)\n",
    "(CHUNK like/VB to/TO see/VB)\n",
    "(CHUNK designed/VBN to/TO provide/VB)\n",
    "(CHUNK get/VB to/TO hear/VB)\n",
    "(CHUNK expects/VBZ to/TO tell/VB)\n",
    "(CHUNK expected/VBN to/TO give/VB)\n",
    "(CHUNK prefer/VB to/TO pay/VB)\n",
    "                \n",
    "                \n",
    "                '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PP Over/IN)\n",
      "  (NP a/DT cup/NN)\n",
      "  (PP of/IN)\n",
      "  (NP coffee/NN)\n",
      "  ,/,\n",
      "  (NP Mr./NNP Stone/NNP)\n",
      "  (VP told/VBD)\n",
      "  (NP his/PRP$ story/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "# 2.6   Representing Chunks: Tags vs Trees\n",
    "# IOB Tags \n",
    "\n",
    "# A token is tagged as B if it marks the beginning of a chunk. Subsequent tokens within the chunk are tagged I. \n",
    "# All other tokens are tagged O. The B and I tags are suffixed with the chunk type, e.g. B-NP, I-NP. Of course, \n",
    "# it is not necessary to specify a chunk type for tokens that appear outside a chunk, so these are just labeled O.\n",
    "\n",
    "from nltk.corpus import conll2000\n",
    "print(conll2000.chunked_sents('train.txt')[99])\n",
    "\n",
    "# Seen below All Chunks in Braces "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  Over/IN\n",
      "  (NP a/DT cup/NN)\n",
      "  of/IN\n",
      "  (NP coffee/NN)\n",
      "  ,/,\n",
      "  (NP Mr./NNP Stone/NNP)\n",
      "  told/VBD\n",
      "  (NP his/PRP$ story/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "print(conll2000.chunked_sents('train.txt', chunk_types=['NP'])[99])\n",
    "\n",
    "# Seen below only - NP in Braces "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  43.4%%\n",
      "    Precision:      0.0%%\n",
      "    Recall:         0.0%%\n",
      "    F-Measure:      0.0%%\n"
     ]
    }
   ],
   "source": [
    "## 3.2   Simple Evaluation and Baselines\n",
    "\n",
    "# Baseline -- Minimum Acceptable Performnce --- We start off by establishing a baseline for the trivial \n",
    "# chunk parser cp that creates no chunks:\n",
    "# No Chunks Defined \n",
    "\n",
    "from nltk.corpus import conll2000\n",
    "cp = nltk.RegexpParser(\"\")\n",
    "test_sents = conll2000.chunked_sents('test.txt', chunk_types=['NP'])\n",
    "print(cp.evaluate(test_sents))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  87.7%%\n",
      "    Precision:     70.6%%\n",
      "    Recall:        67.8%%\n",
      "    F-Measure:     69.2%%\n"
     ]
    }
   ],
   "source": [
    "# Regular expression chunker that looks for tags beginning with letters that are characteristic \n",
    "# of noun phrase tags (e.g. CD, DT, and JJ).\n",
    "\n",
    "grammar = r\"NP: {<[CDJNP].*>+}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "print(cp.evaluate(test_sents))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Verbatim Source -- http://www.nltk.org/book/ch07.html\n",
    "\n",
    "As you can see, this approach achieves decent results. However, we can improve on it by adopting a more data-driven\n",
    "approach, where we use the training corpus to find the chunk tag (I, O, or B) that is most likely for each\n",
    "part-of-speech tag. In other words, we can build a chunker using a unigram tagger (4). \n",
    "But rather than trying to determine the correct part-of-speech tag for each word, we are trying to determine\n",
    "the correct chunk tag, given each word's part-of-speech tag.\n",
    "\n",
    "In 3.1, we define the UnigramChunker class, which uses a unigram tagger to label sentences with chunk tags. \n",
    "Most of the code in this class is simply used to convert back and forth between the chunk tree representation\n",
    "used by NLTK's ChunkParserI interface, and the IOB representation used by the embedded tagger. The class defines \n",
    "two methods: a constructor [1] which is called \n",
    "when we build a new UnigramChunker; and the parse method [3] which is used to chunk new sentences.\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class UnigramChunker(nltk.ChunkParserI):\n",
    "    def __init__(self, train_sents):     # INIT + Self + train Sentences from the TRAIN Data Set \n",
    "        train_data = [[(t,c) for w,t,c in nltk.chunk.tree2conlltags(sent)] ### It first converts training data to a form that is suitable for training the tagger, \n",
    "                                                                           ### using tree2conlltags to map each chunk tree to a list of word,tag,chunk triples\n",
    "                      for sent in train_sents]         # Loops over each \"Sent\" in the \"train_sents\"\n",
    "        self.tagger = nltk.UnigramTagger(train_data)\n",
    "\n",
    "    def parse(self, sentence): \n",
    "        pos_tags = [pos for (word,pos) in sentence]\n",
    "        tagged_pos_tags = self.tagger.tag(pos_tags)\n",
    "        chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]\n",
    "        conlltags = [(word, pos, chunktag) for ((word,pos),chunktag)\n",
    "                     in zip(sentence, chunktags)]\n",
    "        return nltk.chunk.conlltags2tree(conlltags)\n",
    "    \n",
    "# The parse method [3] takes a tagged sentence as its input, and begins by extracting the part-of-speech tags\n",
    "# from that sentence. It then tags the part-of-speech tags with IOB chunk tags, using the tagger self.tagger\n",
    "# that was trained in the constructor. Next, it extracts the chunk tags, and combines them with the original \n",
    "# sentence, to yield conlltags. Finally, it uses conlltags2tree to convert the result back into a chunk tree.    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[('The', 'DT'), ('U.S.', 'NNP'), ('is', 'VBZ'), ('one', 'CD'), ('of', 'IN'), ('the', 'DT'), ('few', 'JJ'), ('industrialized', 'VBN'), ('nations', 'NNS'), ('that', 'WDT'), ('*T*-7', '-NONE-'), ('does', 'VBZ'), (\"n't\", 'RB'), ('have', 'VB'), ('a', 'DT'), ('higher', 'JJR'), ('standard', 'NN'), ('of', 'IN'), ('regulation', 'NN'), ('for', 'IN'), ('the', 'DT'), ('smooth', 'JJ'), (',', ','), ('needle-like', 'JJ'), ('fibers', 'NNS'), ('such', 'JJ'), ('as', 'IN'), ('crocidolite', 'NN'), ('that', 'WDT'), ('*T*-1', '-NONE-'), ('are', 'VBP'), ('classified', 'VBN'), ('*-5', '-NONE-'), ('as', 'IN'), ('amphobiles', 'NNS'), (',', ','), ('according', 'VBG'), ('to', 'TO'), ('Brooke', 'NNP'), ('T.', 'NNP'), ('Mossman', 'NNP'), (',', ','), ('a', 'DT'), ('professor', 'NN'), ('of', 'IN'), ('pathlogy', 'NN'), ('at', 'IN'), ('the', 'DT'), ('University', 'NNP'), ('of', 'IN'), ('Vermont', 'NNP'), ('College', 'NNP'), ('of', 'IN'), ('Medicine', 'NNP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# Named Entity Recognition \n",
    "\n",
    "sent1 = nltk.corpus.treebank.tagged_sents()[22]\n",
    "print(type(sent1))\n",
    "print(sent1)\n",
    "#print(nltk.ne_chunk(sent1, binary=True)) #OK --- Binary True gives \"NE\" only \n",
    "#print(nltk.ne_chunk(sent1)) #OK --- Binary True gives \"PERSON\" \"GPE\" etc  \n",
    "\n",
    "\n",
    "##\n",
    "#\n",
    "\n",
    "\n",
    "#print(nltk.ne_chunk(test)) #--- Binary True gives \"NE\" only "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Own Experi -----\n",
    "\n",
    "def ie_preprocess(docu):\n",
    "    sentences = nltk.sent_tokenize(docu)\n",
    "    sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "    #sentences1 = [nltk.pos_tag(sent) for sent in sentences]\n",
    "    sentences2 = [nltk.UnigramTagger(sent) for sent in sentences]\n",
    "    \n",
    "    return sentences2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-6551d9153d47>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdocu4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"A list of priority infrastructure projects prepared for Trump includes on green energy\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mie_preprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocu1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"  \"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mie_preprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocu2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-647fec4318f0>\u001b[0m in \u001b[0;36mie_preprocess\u001b[0;34m(docu)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m#sentences1 = [nltk.pos_tag(sent) for sent in sentences]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0msentences2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnigramTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msentences2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-647fec4318f0>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m#sentences1 = [nltk.pos_tag(sent) for sent in sentences]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0msentences2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnigramTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msentences2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dhankar/anaconda2/envs/py35/lib/python3.5/site-packages/nltk/tag/sequential.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, train, model, backoff, cutoff, verbose)\u001b[0m\n\u001b[1;32m    338\u001b[0m                  backoff=None, cutoff=0, verbose=False):\n\u001b[1;32m    339\u001b[0m         NgramTagger.__init__(self, 1, train, model,\n\u001b[0;32m--> 340\u001b[0;31m                              backoff, cutoff, verbose)\n\u001b[0m\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode_json_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dhankar/anaconda2/envs/py35/lib/python3.5/site-packages/nltk/tag/sequential.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, n, train, model, backoff, cutoff, verbose)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcutoff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode_json_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dhankar/anaconda2/envs/py35/lib/python3.5/site-packages/nltk/tag/sequential.py\u001b[0m in \u001b[0;36m_train\u001b[0;34m(self, tagged_corpus, cutoff, verbose)\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0mfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConditionalFreqDist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtagged_corpus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m             \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m                 \u001b[0;31m# Record the event.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "# Own Experi -----\n",
    "\n",
    "docu1 = \"George W Bush is a Fine Gentelman\" # Just a STRING\n",
    "docu2 = \"The Lady is a TRAMP\"\n",
    "docu3 = \" The first person who saw the dead men lying in the clearing was Captain Sprout\"\n",
    "docu4 = \"A list of priority infrastructure projects prepared for Trump includes on green energy\"\n",
    "\n",
    "print(ie_preprocess(docu1))\n",
    "print(\"  \"*100)\n",
    "print(ie_preprocess(docu2))\n",
    "print(\"  \"*100)\n",
    "print(ie_preprocess(docu3))\n",
    "print(\"  \"*100)\n",
    "print(ie_preprocess(docu4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TAG's with POS Tagger \n",
    "\n",
    "[[('George', 'NNP'), ('W', 'NNP'), ('Bush', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('Fine', 'JJ'), ('Gentelman', 'NNP')]]\n",
    "                                                                                                                                                                                                        \n",
    "[[('The', 'DT'), ('Lady', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('TRAMP', 'NN')]]\n",
    "                                                                                                                                                                                                        \n",
    "[[('The', 'DT'), ('first', 'JJ'), ('person', 'NN'), ('who', 'WP'), ('saw', 'VBD'), ('the', 'DT'), ('dead', 'JJ'), ('men', 'NNS'), ('lying', 'VBG'), ('in', 'IN'), ('the', 'DT'), ('clearing', 'NN'), ('was', 'VBD'), ('Captain', 'NNP'), ('Sprout', 'NNP')]]\n",
    "                                                                                                                                                                                                        \n",
    "[[('A', 'DT'), ('list', 'NN'), ('of', 'IN'), ('priority', 'NN'), ('infrastructure', 'NN'), ('projects', 'NNS'), ('prepared', 'VBD'), ('for', 'IN'), ('Trump', 'NNP'), ('includes', 'VBZ'), ('on', 'IN'), ('green', 'JJ'), ('energy', 'NN')]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nltk.corpus.reader.util.ConcatenatedCorpusView'>\n",
      "[',', '.', 'accomplished', 'analytically', 'appear', 'apt', 'associated', 'assuming', 'became', 'become', 'been', 'began', 'call', 'called', 'carefully', 'chose', 'classified', 'colorful', 'composed', 'contain', 'differed', 'difficult', 'encountered', 'enough', 'equate', 'extremely', 'found', 'happens', 'have', 'ignored', 'in', 'involved', 'more', 'needed', 'nightly', 'observed', 'of', 'on', 'out', 'quite', 'represent', 'responsible', 'revamped', 'seclude', 'set', 'shortened', 'sing', 'sounded', 'stated', 'still', 'sung', 'supported', 'than', 'to', 'when', 'work']\n"
     ]
    }
   ],
   "source": [
    "# 2.8   Exploring Tagged Corpora\n",
    "# Finding words that APPEAR afer another Word \n",
    "\n",
    "from nltk.corpus import brown\n",
    "\n",
    "brown_learned_text = brown.words(categories='learned')\n",
    "#\n",
    "print(type(brown_learned_text))  # http://www.nltk.org/api/nltk.corpus.reader.html\n",
    "# <class 'nltk.corpus.reader.util.ConcatenatedCorpusView'>\n",
    "#\n",
    "print(sorted(set(b for (a, b) in nltk.bigrams(brown_learned_text) if a == 'often')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'words'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-730f3713184d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtext1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"we are often told that often is a word which will appear often in this text also after often shall occur many other words  \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtext2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategories\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'learned'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbigrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'often'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'words'"
     ]
    }
   ],
   "source": [
    "# Experi own -- Fail as we dont have a -- <class 'nltk.corpus.reader.util.ConcatenatedCorpusView'>\n",
    "\n",
    "text1 = \"we are often told that often is a word which will appear often in this text also after often shall occur many other words  \"\n",
    "\n",
    "text2 = text1.words(categories='learned')  ### AttributeError: 'str' object has no attribute 'words'\n",
    "sorted(set(b for (a, b) in nltk.bigrams(text1) if a == 'often'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-17-0dcf545f4eb6>, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-17-0dcf545f4eb6>\"\u001b[0;36m, line \u001b[0;32m12\u001b[0m\n\u001b[0;31m    for tagged_sent in brown.tagged_sents():\u001b[0m\n\u001b[0m                                            ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "# Next, let's look at some larger context, and find words involving particular sequences \n",
    "# of tags and words (in this case \"<Verb> to <Verb>\"). In code-three-word-phrase we consider\n",
    "# each three-word window in the sentence [1], and check if they meet our criterion [2]. \n",
    "# If the tags match, we print the corresponding words [3].\n",
    "\n",
    "from nltk.corpus import brown\n",
    "def process(sentence):\n",
    "    for (w1,t1), (w2,t2), (w3,t3) in nltk.trigrams(sentence):             ### [1]\n",
    "        if (t1.startswith('V') and t2 == 'TO' and t3.startswith('V')):    ### [2]\n",
    "            #print(w1, w2, w3)                ### [3] --- Works OK -- Dont Uncomment Large Dump .... #### DHANK \n",
    "\n",
    "for tagged_sent in brown.tagged_sents():\n",
    "    process(tagged_sent)\n",
    "    \n",
    "    # \n",
    "    '''\n",
    "    combined to achieve\n",
    "continue to place\n",
    "serve to protect\n",
    "wanted to wait\n",
    "allowed to place\n",
    "expected to become\n",
    "expected to approve\n",
    "expected to make\n",
    "intends to make\n",
    "seek to set\n",
    "like to see\n",
    "designed to provide\n",
    "get to hear\n",
    "expects to tell\n",
    "expected to give\n",
    "prefer to pay\n",
    "required to obtain\n",
    "permitted to teach\n",
    "designed to reduce\n",
    "Asked to elaborate\n",
    "\n",
    "'''\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exposure to group\n",
      "respite to hurry\n",
      "urgings to date\n"
     ]
    }
   ],
   "source": [
    "# Noun to Noun \n",
    "\n",
    "from nltk.corpus import brown\n",
    "def process(sentence):\n",
    "    for (w1,t1), (w2,t2), (w3,t3) in nltk.trigrams(sentence):             ### [1]\n",
    "        if (t1.startswith('NN') and t2 == 'TO' and t3.startswith('NN')):    ### [2]\n",
    "            print(w1, w2, w3)                ### [3] --- Works OK -- Dont Uncomment Large Dump .... #### DHANK \n",
    "\n",
    "for tagged_sent in brown.tagged_sents():\n",
    "    process(tagged_sent)\n",
    "    \n",
    "    #  3 Only ? \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  Once we start doing part-of-speech tagging, we will be creating programs \n",
    "# that assign a tag to a word, the tag which is most likely in a given context #\n",
    "## CONTEXT is Important \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "<class 'dict'>\n",
      "{'colorless': 'ADJ'}\n",
      "{'colorless': 'ADJ', 'sleep': 'V', 'ideas': 'N', 'furiously': 'ADV'}\n",
      "['colorless', 'sleep', 'ideas', 'furiously']\n",
      "<class 'list'>\n",
      "['colorless', 'furiously', 'ideas', 'sleep']\n",
      "['ideas']\n",
      "['furiously']\n",
      "['colorless', 'sleep', 'ideas', 'furiously']\n",
      "['ADJ', 'V', 'N', 'ADV']\n",
      "[('colorless', 'ADJ'), ('sleep', 'V'), ('ideas', 'N'), ('furiously', 'ADV')]\n",
      "[('colorless', 'ADJ'), ('sleep', ['N', 'V']), ('ideas', 'N'), ('furiously', 'ADV')]\n"
     ]
    }
   ],
   "source": [
    "# Py DICTS \n",
    "\n",
    "pos = {}\n",
    "print(pos)\n",
    "print(type(pos))            # <class 'dict'>\n",
    "pos['colorless'] = 'ADJ'\n",
    "print(pos)\n",
    "pos['ideas'] = 'N'\n",
    "pos['sleep'] = 'V'\n",
    "pos['furiously'] = 'ADV'\n",
    "print(pos)\n",
    "#\n",
    "print(list(pos))           # Dict to LIST \n",
    "pos_list = list(pos)\n",
    "print(type(pos_list))      # <class 'list'>\n",
    "print(sorted(pos))\n",
    "print([w for w in pos if w.endswith('as')])\n",
    "print([w for w in pos if w.startswith('fu')])\n",
    "# \n",
    "# dictionary methods keys(), values() and items() \n",
    "# \n",
    "print(list(pos.keys()))\n",
    "#\n",
    "print(list(pos.values()))\n",
    "#\n",
    "print(list(pos.items()))\n",
    "#\n",
    "pos['sleep'] = ['N', 'V'] # Sleep == Noun + Verb , here the ['N', 'V'] IS LIST ASSIGNED AS VALUES IN KEY VALUES for DICT\n",
    "#\n",
    "print(list(pos.items()))\n",
    "#In fact, this is what we saw in 4 for the CMU Pronouncing Dictionary, which stores multiple pronunciations for a single word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-994798c788d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ideas'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'blogs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'adventures'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'N'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "# We have seen above that DICT - VALUES can be LISTS like --  ['N', 'V']\n",
    "# BUT we cant have LISTS as KEY's . LISTS are MUTABLE ...Any Data Structure that is MUTABLE is NOT HASHABLE\n",
    "# Note that dictionary keys must be immutable types, such as strings and tuples. \n",
    "# If we try to define a dictionary using a mutable key, we get a TypeError: unhashable type: 'list'\n",
    "\n",
    "\n",
    "pos = {['ideas', 'blogs', 'adventures']: 'N'}\n",
    "\n",
    "# http://stackoverflow.com/questions/6754102/typeerror-unhashable-type\n",
    "# http://stackoverflow.com/questions/6754102/typeerror-unhashable-type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TBD -- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TBD -- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TBD -- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TBD -- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TBD -- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NN'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4   Automatic Tagging\n",
    "#\n",
    "from nltk.corpus import brown\n",
    "brown_tagged_sents = brown.tagged_sents(categories='news')\n",
    "brown_sents = brown.sents(categories='news')\n",
    "# \n",
    "# We find the Most Frequent TAG is - NN ( NOUN) - noun, singular ... and Assign it as the DEFAULT TAG \n",
    "#\n",
    "tags = [tag for (word, tag) in brown.tagged_words(categories='news')]\n",
    "nltk.FreqDist(tags).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'NN'),\n",
       " ('do', 'NN'),\n",
       " ('not', 'NN'),\n",
       " ('like', 'NN'),\n",
       " ('green', 'NN'),\n",
       " ('eggs', 'NN'),\n",
       " ('and', 'NN'),\n",
       " ('ham', 'NN'),\n",
       " (',', 'NN'),\n",
       " ('I', 'NN'),\n",
       " ('do', 'NN'),\n",
       " ('not', 'NN'),\n",
       " ('like', 'NN'),\n",
       " ('them', 'NN'),\n",
       " ('Sam', 'NN'),\n",
       " ('I', 'NN'),\n",
       " ('am', 'NN'),\n",
       " ('!', 'NN')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "raw = 'I do not like green eggs and ham, I do not like them Sam I am!'\n",
    "tokens = word_tokenize(raw)\n",
    "default_tagger = nltk.DefaultTagger('NN')\n",
    "default_tagger.tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 4.2   The Regular Expression Tagger\n",
    "# \n",
    "# Note that these are processed in order, and the first one that matches is applied.\n",
    "#\n",
    "\n",
    "patterns = [\n",
    "    (r'.*ing$', 'VBG'),               # gerunds\n",
    "    (r'.*ed$', 'VBD'),                # simple past\n",
    "     (r'.*es$', 'VBZ'),                # 3rd singular present\n",
    "     (r'.*ould$', 'MD'),               # modals\n",
    "     (r'.*\\'s$', 'NN$'),               # possessive nouns\n",
    "     (r'.*s$', 'NNS'),                 # plural nouns\n",
    "     (r'^-?[0-9]+(.[0-9]+)?$', 'CD'),  # cardinal numbers\n",
    "     (r'.*', 'NN')                     # nouns (default)\n",
    " ]\n",
    "\n",
    "## DHANKAR--- \n",
    "# Seen above -- The final regular expression «.*» is a catch-all that tags everything as a noun.\n",
    "# Similary in ELIZA or any other BOT the Last Regex Search is a CATCH ALL -- after all other possibilities have been \n",
    "# explored --- we cant have the CATCH All come earlier as it will CATCH All and leave nothing for others . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20326391789486245"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regexp_tagger = nltk.RegexpTagger(patterns)\n",
    "regexp_tagger.tag(brown_sents[3])\n",
    "regexp_tagger.evaluate(brown_tagged_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45578495136941344"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4.3   The Lookup Tagger\n",
    "\n",
    "'''\n",
    "A lot of high-frequency words do not have the NN tag. Let's find the hundred most frequent words and store their most likely tag. \n",
    "We can then use this information as the model for a \"lookup tagger\" (an NLTK UnigramTagger):\n",
    "'''\n",
    "\n",
    "fd = nltk.FreqDist(brown.words(categories='news'))\n",
    "cfd = nltk.ConditionalFreqDist(brown.tagged_words(categories='news'))\n",
    "most_freq_words = fd.most_common(100)\n",
    "\n",
    "likely_tags = dict((word, cfd[word].max()) for (word, _) in most_freq_words)     ####  HOW ??? \n",
    "\n",
    "baseline_tagger = nltk.UnigramTagger(model=likely_tags)\n",
    "baseline_tagger.evaluate(brown_tagged_sents)      # Eval tagger against the BROWN Corpus - tagged Sentences \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def performance(cfd, wordlist):\n",
    "    lt = dict((word, cfd[word].max()) for word in wordlist)\n",
    "    baseline_tagger = nltk.UnigramTagger(model=lt, backoff=nltk.DefaultTagger('NN'))\n",
    "    return baseline_tagger.evaluate(brown.tagged_sents(categories='news'))\n",
    "\n",
    "def display():\n",
    "    import pylab\n",
    "    word_freqs = nltk.FreqDist(brown.words(categories='news')).most_common()\n",
    "    words_by_freq = [w for (w, _) in word_freqs]\n",
    "    cfd = nltk.ConditionalFreqDist(brown.tagged_words(categories='news'))\n",
    "    sizes = 2 ** pylab.arange(15)\n",
    "    perfs = [performance(cfd, words_by_freq[:size]) for size in sizes]\n",
    "    pylab.plot(sizes, perfs, '-bo')\n",
    "    pylab.title('Lookup Tagger Performance with Varying Model Size')\n",
    "    pylab.xlabel('Model Size')\n",
    "    pylab.ylabel('Performance')\n",
    "    pylab.show()\n",
    "    \n",
    "    # DHANK -- Check why not plotting ?? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Various', 'JJ'), ('of', 'IN'), ('the', 'AT'), ('apartments', 'NNS'), ('are', 'BER'), ('of', 'IN'), ('the', 'AT'), ('terrace', 'NN'), ('type', 'NN'), (',', ','), ('being', 'BEG'), ('on', 'IN'), ('the', 'AT'), ('ground', 'NN'), ('floor', 'NN'), ('so', 'QL'), ('that', 'CS'), ('entrance', 'NN'), ('is', 'BEZ'), ('direct', 'JJ'), ('.', '.')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9349006503968017"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# N Gram -- 5.1   Unigram Tagging\n",
    "#\n",
    "\n",
    "from nltk.corpus import brown\n",
    "\n",
    "brown_tagged_sents = brown.tagged_sents(categories='news')\n",
    "brown_sents = brown.sents(categories='news')\n",
    "unigram_tagger = nltk.UnigramTagger(brown_tagged_sents)\n",
    "print(unigram_tagger.tag(brown_sents[2007]))\n",
    "\n",
    "#\n",
    "\n",
    "unigram_tagger.evaluate(brown_tagged_sents)\n",
    "#\n",
    "# 0.9349006503968017 --- As both TRAIN and TEST are TRAIN ONLY -- We need to create a TEST from BROWN CORPUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the Brown Tagged Sentences = 4160\n",
      "<class 'nltk.corpus.reader.util.ConcatenatedCorpusView'>\n",
      "0.8130170437556065\n"
     ]
    }
   ],
   "source": [
    "size = int(len(brown_tagged_sents) * 0.9)\n",
    "print(\"Length of the Brown Tagged Sentences =\",size)\n",
    "\n",
    "print(type(brown_tagged_sents)) # NLTK Specific Data Struct:- <class 'nltk.corpus.reader.util.ConcatenatedCorpusView'>\n",
    "\n",
    "train_sents = brown_tagged_sents[:size]\n",
    "test_sents = brown_tagged_sents[size:]\n",
    "unigram_tagger = nltk.UnigramTagger(train_sents)\n",
    "print(unigram_tagger.evaluate(test_sents))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TBD -------- \n",
    "#\n",
    "# Backoff is a method for combining models: when a more specialized model (such as a bigram tagger) cannot \n",
    "# assign a tag in a given context, we backoff to a more general model (such as a unigram tagger)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
